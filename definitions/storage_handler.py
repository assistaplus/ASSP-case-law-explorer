from os.path import basename, dirname, abspath, join, exists, relpath, isfile
from os import makedirs, getenv, remove, listdir
import boto3
from botocore.exceptions import ClientError
import logging
from dotenv import load_dotenv
load_dotenv()
import sys

"""
Purpose of script:
a)  Define directory paths.
b)  Define the terminology to be used throughout all data processing steps.
    The original terms used in the raw data of "Rechtspraak" and "Legal intelligence" are mapped to each other
    and replaced by a global label.
    Variable names correspond to the original terms and are denoted with the prefix of their source (RS = Rechtspraak, 
    LI = Legal Intelligence). Variables without prefix don't exist in the raw data and are generated by script.
"""

# URL DEFINITIONS:
URL_LI_ENDPOINT = 'https://api.legalintelligence.com'
URL_RS_ARCHIVE = 'http://static.rechtspraak.nl/PI/OpenDataUitspraken.zip'
URL_RS_ARCHIVE_SAMPLE = 'https://surfdrive.surf.nl/files/index.php/s/WaEWoCfKlaS0gD0/download'

# local data folder structure
DIR_ROOT = dirname(dirname(abspath(__file__)))
DIR_DATA = join(DIR_ROOT, 'data')
DIR_DATA_RAW = join(DIR_DATA, 'raw')
DIR_DATA_PROCESSED = join(DIR_DATA, 'processed')
DIR_RECHTSPRAAK = join(DIR_DATA, 'OpenDataUitspraken')

# raw data
CSV_RS_CASES = join(DIR_DATA_RAW, 'RS_cases.csv')
CSV_RS_OPINIONS = join(DIR_DATA_RAW, 'RS_opinions.csv')
CSV_LI_CASES = join(DIR_DATA_RAW, 'LI_cases.csv')
CSV_CASE_CITATIONS = join(DIR_DATA_RAW, 'caselaw_citations.csv')
CSV_LEGISLATION_CITATIONS = join(DIR_DATA_RAW, 'legislation_citations.csv')

# processed data
CSV_RS_CASES_PROC = join(DIR_DATA_PROCESSED, 'RS_cases_clean.csv')
CSV_RS_OPINIONS_PROC = join(DIR_DATA_PROCESSED, 'RS_opinions_clean.csv')
CSV_LI_CASES_PROC = join(DIR_DATA_PROCESSED, 'LI_cases_clean.csv')
CSV_CASE_CITATIONS_PROC = join(DIR_DATA_PROCESSED, 'caselaw_citations_clean.csv')
CSV_LEGISLATION_CITATIONS_PROC = join(DIR_DATA_PROCESSED, 'legislation_citations_clean.csv')

# list of ECLIs (output of RS extractor, input to LIDO extractor)
CSV_RS_ECLIS = join(DIR_DATA, 'RS_eclis.csv')


class Storage:
    def __init__(self, location):
        if location not in ('local', 'aws'):
            print('Storage location must be either "local" or "aws". Setting to "local".')
            location = 'local'
        self.location = location
        self.s3_bucket = None
        self.s3_client = None
        self.setup()

    def setup(self):
        if self.location == 'local':
            # create local data folder structure, if it doesn't exist yet
            for d in [DIR_DATA, DIR_DATA_RAW, DIR_DATA_PROCESSED]:
                makedirs(d, exist_ok=True)

        elif self.location == 'aws':
            # create an S3 bucket in the region of the configured AWS IAM user account
            try:
                region = getenv('AWS_REGION')
                s3_client = boto3.client('s3', region_name=region)
                aws_location = {'LocationConstraint': region}
                s3_client.create_bucket(Bucket=basename(DIR_ROOT), CreateBucketConfiguration=aws_location)
            except ClientError as e:
                if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou' or \
                        e.response['Error']['Code'] == 'BucketAlreadyExists':
                    logging.warning(f'S3 bucket "{basename(DIR_ROOT)}" already exists. Content might be overwritten.')
                else:
                    logging.error(e)
            self.s3_bucket = boto3.resource('s3').Bucket(basename(DIR_ROOT))
            self.s3_client = boto3.client('s3')

    def update(self, path):
        # @TODO: upload by date
        def upload_to_aws(data_path):
            if isfile(data_path):
                try:
                    self.s3_bucket.upload_file(data_path, relpath(data_path, DIR_ROOT))
                except ClientError as e:
                    logging.error(e)
            else:
                for sub_path in listdir(data_path):
                    upload_to_aws(join(data_path, sub_path))

        if self.location == 'aws':
            upload_to_aws(path)
            #remove(path)

    def fetch(self, path):
        msg = path + ' does not exist! ' \
                     'Consider switching input storage location or re-running earlier steps of the pipeline.'

        if self.location == 'local':
            if not exists(path):
                logging.error(msg)
                sys.exit(2)

        elif self.location == 'aws':
            if exists(path):
                logging.warning(f'{path} already exists. Local content will be overwritten by aws content.')
            paginator = self.s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=self.s3_bucket.name, Prefix=relpath(path, DIR_ROOT))
            for page in pages:
                if 'Contents' in page:
                    for obj in page['Contents']:

                        self.s3_bucket.download_file(obj['Key'], path)
                else:
                    logging.error(msg)
                    sys.exit(2)

    def last_updated(self, path, date_attribute_name=''):
        from datetime import date

        def default():
            logging.error('Could not retrieve most recent decision date. Setting start date to 1900-01-01.')
            return date(1900, 1, 1)

        def last_updated_local():
            if path.endswith('.csv'):
                import pandas as pd
                try:
                    df = pd.read_csv(path, usecols=[date_attribute_name])
                    return date.fromisoformat(sorted(list(df[date_attribute_name]))[-1])
                except ValueError:
                    print(f'Attribute {date_attribute_name} not in {path}. Choose valid decision date attribute name.')
                except FileNotFoundError:
                    return default()
            elif path == DIR_RECHTSPRAAK:
                try:
                    last_year = max(list(map(int, listdir(DIR_RECHTSPRAAK))))
                    last_month = int(str(max(list(map(int, listdir(join(DIR_RECHTSPRAAK, str(last_year)))))))[-2:])
                    return date(last_year, last_month, 1)
                except FileNotFoundError:
                    return default()
            return default()

        def last_updated_aws():
            if path.endswith('.csv'):
                self.fetch(path)
                return last_updated_local()
            elif path == DIR_RECHTSPRAAK:
                last_year = 1900
                for year in range(date.today().year, 1899, -1):
                    no_results = self.s3_client.list_objects_v2(
                        Bucket=basename(DIR_ROOT), MaxKeys=1,
                        Prefix=join(relpath(DIR_RECHTSPRAAK, DIR_ROOT), str(year)))['KeyCount']
                    if no_results > 0:
                        last_year = year
                        break
                for month in range(1, 13):
                    yearmonth = f'{str(last_year)}{month:02d}'
                    no_results = self.s3_client.list_objects_v2(
                        Bucket=basename(DIR_ROOT), MaxKeys=1,
                        Prefix=join(relpath(DIR_RECHTSPRAAK, DIR_ROOT), str(last_year), yearmonth))['KeyCount']
                    if no_results > 0:
                        return date(last_year, month, 1)
            return default()

        if not (path.endswith('.csv') or path == DIR_RECHTSPRAAK):
            logging.error(f'Not a valid data path. '
                          f'Choose "{DIR_RECHTSPRAAK}" or path to csv file with decision date attribute name.')
            return default()

        if self.location == 'local':
            return last_updated_local()

        elif self.location == 'aws':
            return last_updated_aws()

        return default()








